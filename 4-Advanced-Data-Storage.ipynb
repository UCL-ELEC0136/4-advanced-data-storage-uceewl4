{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div width=50% style=\"display: block; margin: auto\">\n",
    "    <img src=\"figures/ucl-logo.svg\" width=100%>\n",
    "</div>\n",
    "\n",
    "\n",
    "### [UCL-ELEC0136 Data Acquisition and Processing Systems 2024]()\n",
    "University College London\n",
    "# Lab 4: Advanced Data Storage - Vector Databases and LLMs\n",
    "\n",
    "\n",
    "<hr width=70% style=\"float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** The content of this Notebook will not be evaluated in the final exam. The goal is to provide you with practical experience with the very trendy subject that LLMs are, for you to know how to use them, and provide you with enough to start your own LLM projects. \n",
    "\n",
    "This lab also serves as a good illustration of what this module is about: how data acquisition, storage, and processing all come together to create inteligent AI-driven applications.\n",
    "\n",
    "### Objectives\n",
    "* Perform CRUD operations on a Pinecone Vector Database.\n",
    "* Use Langchain to make queries to API accessed pre-trained LLM models (from Hugging Face and OpenAI).\n",
    "* Compare the performances of two pre-trained LLMs.\n",
    "* Use a Pinecone vector Database to perform Retrieval Augmentation of a pre-trained LLM, allowing it to both expend his knowledge base, and cite sources.\n",
    "\n",
    "### Outline\n",
    "\n",
    "This notebook has 3 parts:\n",
    "\n",
    "0. [Setting up](#0.-Setting-up)\n",
    "1. [CRUD operations on a vector database](#1-crud-operations-on-a-pinecone-vector-database)\n",
    "2. [Intro to LLMs and LangChain](#2.-Intro-to-LLMs-and-Langchain)\n",
    "3. [Retrieval Augmentation of a LLM using a vector database](#3.-Retrieval-Augmentation-of-a-LLM-using-a-vector-database)\n",
    "\n",
    "<hr width=70% style=\"float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- The assignment repository contains a `requirements.txt` file, make sure to install all the librairies with the correct versions listed in this file in your daps conda environment.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Create a Pinecone account and connect to your free-tier online vector database\n",
    "\n",
    "[Pinecone](https://www.pinecone.io) is a vector database service that helps developers build and deploy applications with high-performance similarity search and recommendation capabilities. It enables efficient storage and retrieval of vector data, making it easier to create personalized experiences and content recommendations in various applications, such as stable diffusion, LLMs chatbox, and many other AI applications.\n",
    "\n",
    "In this Notebook, we will use Pinecone's free tier to create and connect to a vector database, perform CRUD operations, and then use that database to power a LLM application.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Follow the instructions in `pinecone_tutorial.pdf` to create a Pinecone account and get an API key for the free tier vector database. MAKE SURE TO KEEP A COPY OF YOUR API KEY.\n",
    "- Run the cell bellow to connect to your online vector database.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# default key\n",
    "38da7e73-57f4-454d-800d-03aad20a2f91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anlly/anaconda3/envs/daps_tmp4lab4/lib/python3.11/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Run this cell (it may take a few seconds)\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   change PINECONE_API_KEY with your pinecone API key and run the cell\n",
    "#\n",
    "###########################\n",
    "\n",
    "PINECONE_API_KEY = \"38da7e73-57f4-454d-800d-03aad20a2f91\" #<--- TODO: your API key here \n",
    "\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=\"gcp-starter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`list_indexes`](https://docs.pinecone.io/reference/list_indexes) function to return the list of Pinecone indexes you have on your database (it should be empty).\n",
    "- If it isn't empty, use the [`delete_index`](https://docs.pinecone.io/reference/delete_index) function to delete any indexes you may have on your database.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['langchain-retrieval-augmentation-fast']\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Check that your pinecone database does not contain any indexes, and delete them if there are any.\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "active_indexes = pinecone.list_indexes()\n",
    "print(active_indexes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Create a HuggingFace account and generate a free API key\n",
    "\n",
    "**Note:** you do not need this step to do part. [# 1. CRUD opperations on a Vector Database](#1.-CRUD-opperations-on-a-Vector-Database).\n",
    "\n",
    "\n",
    "[Hugging Face ü§ó](https://huggingface.co) is a company and open-source platform that specializes in natural language processing (NLP) and provides tools, libraries, and pre-trained models for building and deploying NLP applications. Their most well-known product is the Transformers library, which offers access to a wide range of pre-trained NLP models, making it easier for developers to work with text-based tasks such as language translation, sentiment analysis, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Follow the instructions in `huggingface_tutorial.pdf` to create a HuggingFace account and get an API key for the free tier vector database. MAKE SURE TO KEEP A COPY OF YOUR API KEY.\n",
    "- Run the cell bellow to set an environment variable to your API key. `langchain.HuggingFaceHub` will use this environment variable when sending a request to the Hugging Face server to authentificate the connection.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token\n",
    "\n",
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "###########################\n",
    "# Task: \n",
    "#   change HUGGING_FACE_API_KEY with your Hugging Face API key and run the cell to set the environment variable HUGGINGFACEHUB_API_TOKEN\n",
    "#\n",
    "###########################\n",
    "\n",
    "HUGGING_FACE_API_KEY = \"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\" #<--- TODO: your Hugging Face API key here \n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HUGGING_FACE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Create an OpenAI account and generate a free API key\n",
    "\n",
    "**Note:** you do not need this step to do part. [1. CRUD opperations on a Vector Database](#1.-crud-operations-on-a-pinecone-vector-database) and part. [2.1 Hugging Face LLM](#211-initializing-the-llm).\n",
    "\n",
    "[OpenAI](https://openai.com) is an artificial intelligence (AI) research laboratory consisting of the for-profit OpenAI LP and its non-profit parent company, OpenAI Inc.\n",
    "\n",
    "OpenAI provides an [API](https://platform.openai.com/docs/overview) that allows developers to access and integrate the capabilities of OpenAI's language models into their own applications, products, or services. The OpenAI API is based on models like GPT and allows developers to make use of powerful natural language processing (NLP) functionalities.\n",
    "\n",
    "**Unlike Hugging Face, OpenAI is not open source, and the free tier of their API is only available for 3 months after the creation of a new account, and has a lot of restriction (for exemple, you are limited to 3 requests per minutes for each model). However, their models are state of the art and very powerful, which is why we will use them in this lab.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Follow the instructions in `openai_tutorial.pdf` to create a HuggingFace account and get an API key for the free tier vector database. MAKE SURE TO KEEP A COPY OF YOUR API KEY.\n",
    "- Run the cell bellow to set an environment variable to your API key. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "key\n",
    "\n",
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   change OPENAI_API_KEY with your OpenAI API key and run the cell to set the environment variable HUGGINGFACEHUB_API_TOKEN\n",
    "#\n",
    "###########################\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = \"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\" #<--- TODO: your OpenAI API key here \n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CRUD operations on a Pinecone Vector Database\n",
    "\n",
    "*Source: https://docs.pinecone.io/docs/quickstart*\n",
    "\n",
    "\n",
    "In this part, we will familiarize ourselve with basic operations on a Pinecode Vector Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>üë©‚Äçüíªüë®‚Äçüíª Optional action</b>\n",
    "\n",
    "- Very short video intro on Vector Databases: [Vector databases are so hot right now. What are they?](https://www.youtube.com/watch?v=klTvEwg3oJ4)\n",
    "- Short read: [Everything you need to know about Pinecone ‚Äì A Vector Database](https://www.packtpub.com/article-hub/everything-you-need-to-know-about-pinecone-a-vector-database).\n",
    "- In depth read: [What is a Vector Database & How Does it Work? Use Cases + Examples](https://www.pinecone.io/learn/vector-database/).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "Once your connection to your Pinecone database is established (which should have been done in part. 0.1), you do not need to use `pinecone.init` anymore and can directly use the API requests.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 CRUD operations on a Vector Database - Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`create_index`](https://docs.pinecone.io/reference/create_index) function to create an index, name it `quickstart`, set the dimension to 8, and use the metric `euclidean`.\n",
    "- Check that your database contains an index called `quickstart`.\n",
    "- Use the [`describe_index`](https://docs.pinecone.io/reference/describe_index) function to get informations about the index `quickstart`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quickstart']\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create an index called quickstart, check that it has been added to your Pinecone Vector DB, and get information about that index\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "pinecone.create_index(\"quickstart\", dimension=8, metric=\"euclidean\")\n",
    "pinecone.describe_index(\"quickstart\")\n",
    "active_indexes = pinecone.list_indexes()\n",
    "print(active_indexes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`Index`](https://docs.pinecone.io/docs/python-client#index) class to contruct an `Index` object from the index `quickstart`. \n",
    "- Use the [`upsert`](https://docs.pinecone.io/docs/python-client#indexupsert) method to push the 5 vectors in the cell bellow to the index `quickstart`.\n",
    "- Use the [`describe_index_stats`](https://docs.pinecone.io/docs/python-client#indexdescribe_index_stats) method to get statistics about the index's contents.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "Wait for a few seconds after you use `upsert` before querying the index with `describe_index_stats` as the data needs a bit of time to be saved.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 8,\n",
      " 'index_fullness': 5e-05,\n",
      " 'namespaces': {'': {'vector_count': 5}},\n",
      " 'total_vector_count': 5}\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Construct an Index object and use it to push the 5 vectors in data to your index, and get statistics about the index\n",
    "#\n",
    "###########################\n",
    "\n",
    "data = [\n",
    "    (\"A\", [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], {\"genre\": \"comedy\", \"year\": 2020}),\n",
    "    (\"B\", [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2], {\"genre\": \"documentary\", \"year\": 2019}),\n",
    "    (\"C\", [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3], {\"genre\": \"comedy\", \"year\": 2019}),\n",
    "    (\"D\", [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], {\"genre\": \"drama\"}),\n",
    "    (\"E\", [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], {\"genre\": \"drama\"})\n",
    "    ]\n",
    "\n",
    "\n",
    "# TODO : your code bellow\n",
    "# construct an index object from the index quickstart\n",
    "index = pinecone.Index(\"quickstart\")\n",
    "index.upsert(vectors=data)\n",
    "index_stats_response = index.describe_index_stats()\n",
    "print(index_stats_response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 CRUD operations on a Vector Database - Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`query`](https://docs.pinecone.io/docs/python-client#indexquery) method to search for the 3 closest vectors to the `target_vector` in the index `quickstart`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'C',\n",
       "              'score': 0.0,\n",
       "              'values': [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]},\n",
       "             {'id': 'D',\n",
       "              'score': 0.0799999237,\n",
       "              'values': [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]},\n",
       "             {'id': 'B',\n",
       "              'score': 0.0800000429,\n",
       "              'values': [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]}],\n",
       " 'namespace': ''}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   find the 3 nearest vectors to the target_vector\n",
    "#\n",
    "###########################\n",
    "target_vector = [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]\n",
    "\n",
    "# TODO : your code bellow\n",
    "index.query(\n",
    "  vector=[0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n",
    "  top_k=3,\n",
    "  include_values=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`query`](https://docs.pinecone.io/docs/python-client#indexquery) method to search for the 3 closest vectors to the `target_vector` the index `quickstart` that are dramas.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'D',\n",
       "              'score': 0.0799999237,\n",
       "              'values': [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]},\n",
       "             {'id': 'E',\n",
       "              'score': 0.319999695,\n",
       "              'values': [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]}],\n",
       " 'namespace': ''}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   find the 3 nearest vectors to the target_vector that are dramas\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "index.query(\n",
    "  vector=[0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n",
    "  top_k=3,\n",
    "  include_values=True,\n",
    "  filter={\n",
    "        'genre': {'$in': ['drama']}\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 CRUD operations on a Vector Database - Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to change the values associated to vectors A and D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': 'A',\n",
      "              'metadata': {'genre': 'comedy', 'year': 2020.0},\n",
      "              'score': 0.0,\n",
      "              'values': [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]}],\n",
      " 'namespace': ''}\n",
      "{'matches': [{'id': 'D',\n",
      "              'metadata': {'genre': 'drama'},\n",
      "              'score': 0.0,\n",
      "              'values': [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]}],\n",
      " 'namespace': ''}\n"
     ]
    }
   ],
   "source": [
    "# Run this cell\n",
    "print(index.query(\n",
    "  id = \"A\",\n",
    "  top_k = 1,\n",
    "  include_values = True,\n",
    "    include_metadata = True\n",
    "))\n",
    "\n",
    "print(index.query(\n",
    "  id = \"D\",\n",
    "  top_k = 1,\n",
    "  include_values = True,\n",
    "  include_metadata = True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`update`](https://docs.pinecone.io/docs/python-client#indexupdate) method to do the following updates:\n",
    "    - Replace all the values of `A` by `0.6`, change the genre to `action-comedy`.\n",
    "    - Replace all the values of `D` by `-0.4`, add a `year` field in the metadata and set it to `2019`.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "Wait for a few seconds after you use `update` before querying the index as the data needs a bit of time to be saved.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': 'A',\n",
      "              'metadata': {'genre': 'action-comedy', 'year': 2020.0},\n",
      "              'score': 0.0,\n",
      "              'values': [0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6]}],\n",
      " 'namespace': ''}\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Update vectors A and D as indicated above.\n",
    "#\n",
    "###########################\n",
    "import time\n",
    "\n",
    "# TODO : your code bellow\n",
    "update_response = index.update(\n",
    "    id=\"A\",\n",
    "    values=[0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6],\n",
    "    set_metadata={'genre': 'action-comedy'},\n",
    ")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "print(index.query(\n",
    "  id = \"A\",\n",
    "  top_k = 1,\n",
    "  include_values = True,\n",
    "  include_metadata = True\n",
    "))\n",
    "\n",
    "update_response = index.update(\n",
    "    id=\"D\",\n",
    "    values=[-0.4, -0.4, -0.4, -0.4, -0.4, -0.4, -0.4, -0.4],\n",
    "    set_metadata={'year': 2019},\n",
    ")\n",
    "\n",
    "time.sleep(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': 'A',\n",
      "              'metadata': {'genre': 'action-comedy', 'year': 2020.0},\n",
      "              'score': 0.0,\n",
      "              'values': [0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6]}],\n",
      " 'namespace': ''}\n",
      "{'matches': [{'id': 'D',\n",
      "              'metadata': {'genre': 'drama', 'year': 2019.0},\n",
      "              'score': 0.0,\n",
      "              'values': [-0.4, -0.4, -0.4, -0.4, -0.4, -0.4, -0.4, -0.4]}],\n",
      " 'namespace': ''}\n"
     ]
    }
   ],
   "source": [
    "# Run this cell\n",
    "print(index.query(\n",
    "  id = \"A\",\n",
    "  top_k = 1,\n",
    "  include_values = True,\n",
    "  include_metadata = True\n",
    "))\n",
    "\n",
    "print(index.query(\n",
    "  id = \"D\",\n",
    "  top_k = 1,\n",
    "  include_values = True,\n",
    "  include_metadata = True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 CRUD operations on a Vector Database - Delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`delete`](https://docs.pinecone.io/docs/python-client#indexdelete) method to delete the vectors `B` and `C`.\n",
    "- Check with `describe_index_stats` that your index now contains 3 vectors\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "Wait for a few seconds after you use `upsert` before querying the index with `describe_index_stats` as the data needs a bit of time to be saved.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Delete vectors B and C from the index.\n",
    "#\n",
    "###########################\n",
    "\n",
    "\n",
    "# TODO : your code bellow\n",
    "delete_response = index.delete(ids=['B', 'C'])\n",
    "\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 8,\n",
       " 'index_fullness': 3e-05,\n",
       " 'namespaces': {'': {'vector_count': 3}},\n",
       " 'total_vector_count': 3}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`delete_index`](https://docs.pinecone.io/reference/delete_index) function to delete the index `quickstart`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   delete the index quickstart from your Pinecode Vector Database\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "pinecone.delete_index('quickstart')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Intro to LLMs and LangChain\n",
    "\n",
    "**Context:**\n",
    "\n",
    "* **Large language models (LLMs)** are a class of artificial intelligence models that have been trained on vast amounts of text data to understand and generate human-like text. These models are based on deep learning techniques, such as neural networks, and have many parameters, often numbering in the hundreds of millions or even billions. Some well-known examples include OpenAI's GPT-3 and GPT-4, and Google's BERT and T5 models.\n",
    "\n",
    "* \" [**LangChain**](https://python.langchain.com/docs/get_started/introduction) is an open source framework that lets software developers working with artificial intelligence (AI) and its machine learning subset combine large language models with other external components to develop LLM-powered applications. The goal of LangChain is to link powerful LLMs, such as OpenAI's GPT-3.5 and GPT-4, to an array of external data sources to create and reap the benefits of natural language processing (NLP) applications. \" - [Source](https://www.techtarget.com/searchenterpriseai/definition/LangChain#:~:text=LangChain%20is%20an%20open%20source,to%20develop%20LLM%2Dpowered%20applications.)\n",
    "\n",
    "\n",
    "\n",
    "**Although we are doing these operations in a Jupyter Notebook, the exact same code can be used to program server-hosted web applications that could perform real-world tasks.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>üë©‚Äçüíªüë®‚Äçüíª Optional action</b>\n",
    "\n",
    "- In depth read: [LangChain AI Handbook](https://www.pinecone.io/learn/series/langchain/).\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "In this part, we will use LangChain to deploy two pre-trained LLMs, one from Hugging Face Hub, and one from OpenAI's API.\n",
    "\n",
    "*Source: [https://www.pinecone.io/learn/series/langchain/langchain-intro/](https://www.pinecone.io/learn/series/langchain/langchain-intro/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Hugging Face LLM\n",
    "\n",
    "Let's use a pre-trained Google language model, [flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl), hosted on the Hugging Face Hub, that we will access for free through Hugging Face's API. More specifically, we will use the HuggingFaceHub module of the langchain library, which will query Hugging Face's API for us.\n",
    "\n",
    "### 2.1.1 Initializing the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cell below to initialize the connection to the LLM.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anlly/anaconda3/envs/daps_tmp4lab4/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Run this cell\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "        repo_id='google/flan-t5-xxl',\n",
    "    model_kwargs={'temperature':1e-2} #Best temperature found: 1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Use LangChain to ask a question to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do to query a LLM is to create a **prompt template**. A prompt template contains instructions to generate a prompt in a reproductible way. It contains a text string (\"the template\"), that can take in a set of parameters from the end user and generates a prompt (the input variables).\n",
    "\n",
    "For example: \n",
    "\n",
    "* We want a LLM model to tell what language a sentence is written in, then an appropriate prompt template would be: \n",
    "    * `\"What language is the sentense \"{sentence}\" written in?\"`\n",
    "    * Here, `sentence` is the only input variable. \n",
    "\n",
    "<br/>\n",
    "\n",
    "* We want a LLM model to generate jokes about a topic, while specifying what type of jokes, then an appropriate prompt template would be: \n",
    "    * `\"Make a {type} of joke about {subject}\"\"`\n",
    "    * Here, `type` and `subject` are the input variables.\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "**Note:** `google/flan-t5-xxl` is a small LLM, best suited to give short answers. Although it is enough for us to explore LLMs today, for a real application bigger models would be better suited.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Using [langchain.PromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html), create a prompt template with an input variable called `name` that asks the language model to give the date of birth of a historical figure. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create a PromptTemplate with an input variable called `name` that asks the language model to give the date of birth of a historical figure.\n",
    "#\n",
    "###########################\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# TODO : your code bellow\n",
    "prompt = PromptTemplate(input_variables=[\"name\"], template=\"Say the DoB of {name}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Using langchain.LLMChain, create a chain to run the prompt by our LLM.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "An LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided, passes the formatted string to LLM and returns the LLM output.\n",
    "\n",
    "**Example:** \n",
    "llm_chain = LLMChain(\n",
    "    prompt=your_PromptTemplate_template,\n",
    "    llm=your_llm\n",
    ")\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create a LLMchain with your prompt and your hub_llm\n",
    "#\n",
    "###########################\n",
    "\n",
    "# from langchain import LLMChain\n",
    "from langchain.chains import LLMChain\n",
    "# cannot use the original package, need to use chains here\n",
    "# also need to install cython and cchardet\n",
    "\n",
    "# TODO : your code bellow\n",
    "llm_chain = LLMChain(prompt=prompt, llm=hub_llm)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Use `LLMChain.run()` or `LLMChain.predict()` to ask for the date of birth of Napoleon.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1806'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Asks the language model to give the date of birth of a historical figure.\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "llm_chain.run(\"Napoleon\")\n",
    "llm_chain.predict(name=\"Napoleon\")  # need to use input variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finnish'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"sentence\"], template=\"What language is the sentence {sentence} written in?\")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=hub_llm)\n",
    "llm_chain.run(\"this is a sentence\")\n",
    "llm_chain.predict(sentence=\"this is a sentence\")  # need to use input variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The math teacher was a little confused when he saw the student's homework.\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"type\",\"subject\"], template=\"Make a {type} of joke about {subject}\")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=hub_llm)\n",
    "llm_chain.predict(type=\"kind\", subject=\"math\")  # need to use input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPOILER:** Well, that's not amazing. The LLM understood we wanted a date, and we even got something close to Napoleon's real date of birth, but the results is wrong. This is because this LLM is small and not adapted to this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 OpenAI LLM\n",
    "\n",
    "In this part, we will task a more powerfull openAI LLM called [`text-davinci-003`](https://platform.openai.com/docs/models/gpt-3), a variant of GPT3,to perform the same task we instructed the Hugging Face hosted model, and see if we get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Initializing the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cell bellow to innitialize the connection to the LLM.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "davinci = OpenAI(model_name='text-davinci-003', openai_api_key =  OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Use LangChain to ask a question to the LLM\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Using langchain.LLMChain, create a chain to run the same prompt as in part. 2.1.2 by our OpenAI LLM.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nNapoleon Bonaparte was born on August 15, 1769.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create a LLMchain with your prompt and your hub_llm\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "prompt = PromptTemplate(input_variables=[\"name\"], template=\"Say the DoB of {name}\")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=davinci)\n",
    "llm_chain.run(\"Napoleon\")\n",
    "llm_chain.predict(name=\"Napoleon\")  # need to use input variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nEnglish'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"sentence\"], template=\"What language is the sentence {sentence} written in?\")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=davinci)\n",
    "llm_chain.run(\"this is a sentence\")\n",
    "llm_chain.predict(sentence=\"this is a sentence\")  # need to use input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nQ: Why was the math book so sad?\\nA: Because it had so many problems!'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"type\",\"subject\"], template=\"Make a {type} of joke about {subject}\")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=davinci)\n",
    "llm_chain.predict(type=\"kind\", subject=\"math\")  # need to use input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Use `LLMChain.run()` or `LLMChain.predict()` to ask for the date of birth of Napoleon.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Asks the language model to give the date of birth of a historical figure.\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPOILER:** Alright that's better, Napoleon was indeed born of August 15, 1769. Let's ask something more complex and recent and see how our LLM does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Using [langchain.PromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html), create a new prompt template with an input variable called `year` that asks the language model to give the winner of the FIFA world cup on a given year.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nUruguay won the first ever FIFA World Cup in 1930.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create a PromptTemplate with an input variable called `year` that asks the language model to give the winner of the FIFA world cup of that year.\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "prompt = PromptTemplate(input_variables=[\"year\"], template=\"please give me the winner of FIFA world cup on {year}\")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=davinci)\n",
    "llm_chain.predict(year=\"1930\")  # need to use input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Using langchain.LLMChain, create a chain to run the new prompt template by our OpenAI LLM.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create a LLMchain with your prompt and your hub_llm\n",
    "#\n",
    "###########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Use `LLMChain.run()` or `LLMChain.predict()` to ask for the winner of the 2022 FIFA world cup.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nIt is too early to predict the winner of the 2022 FIFA World Cup, as the tournament has not yet taken place.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Asks the language model to ask for the winner of the 2022 FIFA world cup\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "llm_chain.predict(year=\"2022\")  # need to use input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM doesn't know what happened in 2022 because it was trained before that year. How can we update the LLM's knowledge to make it up to date? With Retrieval augmentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Retrieval Augmentation of a LLM using a Vector Database\n",
    "\n",
    "**WARNING:** Some of the cells in this part may take some time to run as we are working with a lot of data.\n",
    "\n",
    "The most powerful LLMs in the world have no idea about recent world events, nor can they cite sources. In general a LLM will only have knowledge about what it has been exposed to during training. For LLMs, the world exists as a static snapshot of the world as it was within their training data. \n",
    "\n",
    "A solution to this problem is **retrieval augmentation**: we retrieve relevant information from an external knowledge base and give that information to our LLM.\n",
    "\n",
    "*Source: https://docs.pinecone.io/docs/langchain#retrieval-augmentation-in-langchain*\n",
    "\n",
    "\n",
    "<div width=50% style=\"display: block; margin: auto\">\n",
    "    <img src=\"figures/augmentation.png\" width=70%>\n",
    "</div>\n",
    "\n",
    "To perform **retrieval augmentation**, we will embed data with an **embedding model**, store the embedded data into a **vector database index**, and create a **LangChain vectorstore** that will use the index and the embedding model to find relevant information to the prompt sent by the user, and feed the most relevant results found in the database to the **LLM** to provide it with context and sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Building a knowledge base with Vector Embedding \n",
    "\n",
    "Source: [Creating the knowledge base](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/#Creating-the-Knowledge-Base)\n",
    "\n",
    "This part concists in taking a dataset of relevant content we want to augment our LLM with (it could be code documentation for an LLM that needs to help write code, company documents for an internal chatbot...), process it, and embedded it into vectors. \n",
    "\n",
    "You can look [here](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/#Creating-the-Knowledge-Base) fore the full processs, which you would have to do if you wanted to augment your LLM for a specific use-case.\n",
    "\n",
    "**For the sake of simplicity, we will use a pre-embembeded dataset from pinecone_dataset and upload it to a Pinecone Vector database. Unfortunatly, this dataset doesn't contain data on the 2022 FIFA world cup, but the process used here can be applied with other datasets, giving you an idea of the process should you want to use it for personal projects.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cell bellow to import and process the pre-embedded data we will use for retrieval augmentation\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "import pinecone_datasets\n",
    "\n",
    "# We import a dataset\n",
    "dataset = pinecone_datasets.load_dataset('wikipedia-simple-text-embedding-ada-002-100K')\n",
    "# We drop sparse_values as they are not needed for this example\n",
    "dataset.documents.drop(['metadata'], axis=1, inplace=True)\n",
    "dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)\n",
    "# We will use rows of the dataset up to index 30_000 to make the upload to the Pinecone Vector Database faster\n",
    "dataset.documents.drop(dataset.documents.index[30_000:], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cell bellow to check that your pinecone database does not contain any indexes, and delete them if there are any.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['langchain-retrieval-augmentation-fast']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pinecone.list_indexes()\n",
    "\n",
    "#pinecone.delete_index(\"index_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Use the [`create_index`](https://docs.pinecone.io/reference/create_index) function to create an index, name it `langchain-retrieval-augmentation-fast`, set the dimension to **1536**, and use the metric `cosine`.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "**Note:** We set the dimension to 1536 as this is the dimension of OpenAI's text embedding model 'text-embedding-ada-002' that we use to embed the data. If you wish to use another model (for instance a Hugging Face model using HuggingFaceInferenceAPIEmbeddings), you will have to change this number to match the dimension of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['langchain-retrieval-augmentation-fast']\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Check that your pinecone database does not contain any indexes, and delete them if there are any.\n",
    "#\n",
    "###########################\n",
    "\n",
    "index_name = 'langchain-retrieval-augmentation-fast'\n",
    "\n",
    "# TODO : your code bellow\n",
    "pinecone.create_index(index_name, dimension=1536, metric=\"cosine\")\n",
    "pinecone.describe_index(index_name)\n",
    "active_indexes = pinecone.list_indexes()\n",
    "print(active_indexes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cell bellow to push the data on to the Vector Database. This can take a few minutes.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.296,\n",
       " 'namespaces': {'': {'vector_count': 29600}},\n",
       " 'total_vector_count': 29600}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell - It may take a few minutes\n",
    "\n",
    "import time\n",
    "\n",
    "index = pinecone.GRPCIndex(index_name) # GRPC allows for faster upserts to the Pinecone Vector Database\n",
    "# wait a moment for the index to be fully initialized\n",
    "time.sleep(1)\n",
    "\n",
    "index.describe_index_stats()\n",
    "\n",
    "for batch in dataset.iter_documents(batch_size=100):\n",
    "    index.upsert(batch)\n",
    "\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Creating a vector store\n",
    "\n",
    "Now that we've build our index we can switch over to LangChain. We need to initialize a LangChain vector store using the same index we just built. For this we will also need a LangChain embedding object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the following cells to initialize a LangChain vector store.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the following cell to perform a `similarity_search` of the content of the query on our [vectorstore](https://python.langchain.com/docs/modules/data_connection/vectorstores/) containing embedded information about our augmentation dataset.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"The French invasion of Russia in 1812 became Napoleon's first big defeat. His army was badly damaged and never fully recovered. In 1813, another Coalition defeated his forces at Leipzig. The year after that, they attacked France and won. The Coalition exiled Napoleon to the island of Elba. Less than a year later, he escaped Elba and briefly returned to be the Emperor of France. However, he was defeated at the Battle of Waterloo in June 1815. Napoleon spent the last six years of his life exiled to the island of Saint Helena, which was controlled by the British, and died at the age of 51. A doctor said he died of stomach cancer. Some scientists think he was poisoned, though others disagree.\\n\\nNapoleon is remembered as a brilliant army leader, and his campaigns are studied at military schools all over the world. People have many different views on whether he was a good or bad ruler. He brought many ideas of liberalism and the French Revolution to the countries he conquered, such as the Napoleonic code, freedom of religion and making education and government more modern. His enemies remembered him as a tyrant and some historians criticise him for causing many wars .\\n\\nBirth and education \\nNapoleon Bonaparte was born in Casa Buonaparte in the town of Ajaccio, Corsica, on the 15th of August 1769. This was one year after the island was given to France by the Republic of Genoa. He was the second of eight children. He was named Napoleone di Buonaparte. He took his first name from an uncle who had been killed fighting the French. However, he later used the more French-sounding Napol√©on Bonaparte.\", metadata={'chunk': 1.0, 'source': 'https://simple.wikipedia.org/wiki/Napoleon', 'title': 'Napoleon', 'wiki-id': '14820'}),\n",
       " Document(page_content=\"Head of the House of Bonaparte \\nAfter the death of his elder brother Joseph in 1844, Louis was seen by Bonapartists as the rightful Emperor of the French, although he took little action himself to advance the claim. His son and heir, Charles Louis-Napoleon Bonaparte, on the other hand, was at that time imprisoned in France for having tried to engineer a Bonapartist coup d'√©tat. Louis died on July 25 1846, making his son the future Napoleon III.\\n\\nReferences \\n\\n Napoleon Bonaparte: A Life, by Alan Schom\\n\\n1779 births\\n1884 deaths\\nHouse of Bonaparte\\nKings and Queens of the Netherlands\\nPeople from Corsica\", metadata={'chunk': 2.0, 'source': 'https://simple.wikipedia.org/wiki/Louis%20Bonaparte', 'title': 'Louis Bonaparte', 'wiki-id': '6594'}),\n",
       " Document(page_content=\"The Corsican Buonapartes were from lower Italian nobility. They had come to Corsica in the 16th century. His father Nobile Carlo Buonaparte became Corsica's representative to the court of Louis XVI in 1777. The greatest influence of Napoleon's childhood was his mother, Maria Letizia Ramolino. Her firm education controlled a wild child. He had an older brother, Joseph. He also had younger siblings Lucien, Elisa, Louis, Pauline, Caroline and J√©r√¥me. Napoleon was baptized as a Catholic just before his second birthday, on 21 July 1770 at Ajaccio Cathedral.\\n\\nAlthough raised a Catholic, Napoleon was a deist.\\n\\nEarly military career \\n\\nNapoleon was able to enter the military academy at Brienne in 1779. He was nine years old when he entered the academy. He moved to the Parisian √âcole Royale Militaire in 1784 and graduated a year later as a second lieutenant of artillery. Napoleon was able to spend much of the next eight years in Corsica. There he played an active part in political and military matters. He came into conflict with the Corsican nationalist Pasquale Paoli, and his family was forced to flee to Marseille in 1793.\\n\\nThe French Revolution caused much fighting and disorder in France. At times, Napoleon was connected to those in power. Other times, he was in jail. In the French Revolutionary Wars he helped the Republic against royalists who supported the former king of France. In September 1793, he assumed command of an artillery brigade at the siege of Toulon, where royalist leaders had welcomed a British fleet and troops. The British were driven out in December 17, 1793, and Bonaparte was rewarded with promotion to brigadier general and assigned to the French army in Italy in February 1794.\", metadata={'chunk': 2.0, 'source': 'https://simple.wikipedia.org/wiki/Napoleon', 'title': 'Napoleon', 'wiki-id': '14820'})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell\n",
    "\n",
    "query = \"When was Napoleon born\"\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query,  # our search query\n",
    "    k=3  # return 3 most relevant docs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Augmented LLM query\n",
    "\n",
    "We finally have all the pieces of the puzzle, and we can now force the LLM to answer a question based on the information it is seeing being returned from the vectorstore. This allows for many things, like giving it up-to-date information, but can also be used to [cite sources](https://python.langchain.com/docs/use_cases/question_answering/vector_db_qa#return-source-documents). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the following cell to create a `VectorStoreRetrieverMemory` object that we will pass on to the LLMChain.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "from langchain.memory.vectorstore import VectorStoreRetrieverMemory\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=1))\n",
    "memory_RAG = VectorStoreRetrieverMemory(retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Create a new prompt template with a question of your choice. For example, you can ask about the life of historical figures.\n",
    "\n",
    "- Using langchain.LLMChain, create a chain to run the new prompt template by our OpenAI LLM davinci. Add to the lists of arguments `memory = memory_RAG` to instruct the LLM to look for the answer in the vectorstore.\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nNapoleon Bonaparte was born in 1769 on the island of Corsica and was a French military and political leader. He rose to power during the French Revolution and was appointed Commander of the French Army in 1796. He led several successful military campaigns in Europe, including the Italian and Egyptian campaigns, and became the Emperor of France in 1804. Napoleon was defeated at the Battle of Waterloo in 1815 and was exiled to the island of St. Helena, where he died in 1821. Napoleon was an influential figure in history and is remembered for his military genius, his reformative policies, and his ambition. He is considered one of the greatest military commanders in history and is credited with modernizing the French army. He is also remembered for his legal reforms, such as the Code Napoleon, which established a unified set of laws across all of France.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Asks the language model to give the date of birth of a historical figure.\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"name\"], template=\"describe the life of {name}\")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=davinci, memory=memory_RAG)\n",
    "llm_chain.predict(name=\"Napoleon\")  # need to use input variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Augmented LLM query with sources\n",
    "\n",
    "As stated before, we can use retrieval augmentation to provide sources to the output of the LLM (provided the sources were in the metadata of the embedded dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Tell me about the life of Napoleon',\n",
       " 'answer': ' Napoleon Bonaparte was born in Casa Buonaparte in the town of Ajaccio, Corsica, on the 15th of August 1769. He was the second of eight children. He is remembered as a brilliant army leader, and his campaigns are studied at military schools all over the world. People have many different views on whether he was a good or bad ruler. He was exiled to the island of Elba, then briefly returned to be the Emperor of France, before being defeated at the Battle of Waterloo. He spent the last six years of his life exiled to the island of Saint Helena, which was controlled by the British, and died at the age of 51. A doctor said he died of stomach cancer, but some scientists think he was poisoned.\\n',\n",
       " 'sources': 'https://simple.wikipedia.org/wiki/Napoleon'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell\n",
    "\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(llm = davinci, chain_type=\"stuff\", retriever=retriever)\n",
    "chain({\"question\": \"Tell me about the life of Napoleon\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>üë©‚Äçüíªüë®‚Äçüíª Optional action</b>\n",
    "\n",
    "- Reading: [Making Retrieval Augmented Generation Fast](https://www.pinecone.io/learn/fast-retrieval-augmented-generation/)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The end\n",
    "\n",
    "That's the end of this lab! We hope you learned a lot through it, and that you are now ready to go on the adventure on your own and explore all that is possible to do with LLMs and vector Databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>üë©‚Äçüíªüë®‚Äçüíª Optional action</b>\n",
    "\n",
    "Explore the other exemple of applications of vector databases listed in Pinecone's documentation:\n",
    "\n",
    "https://docs.pinecone.io/page/examples\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit this assignment and **every other future assignment**, included the **final assignment** you have to:\n",
    "- Commit and push your code to GitHub\n",
    "- Go to **your** repository of the assignment. This must be on our course organisation `UCL-ELEC0136` and usually has the pattern `https://github.com/UCL-ELEC0136/<assignment-name>-<your-github-username>`.\n",
    "- Go in the `Pull requests` tab and click on the `Feedback` pull request.\n",
    "- Click on `Files changed` and verify that the files you have changed are listed.\n",
    "- Merge the pull request by clicking on `Merge pull request` and then `Confirm merge`.\n",
    "\n",
    "We are now ready to push our code that acquires data from GitHub to our repository (which is also GitHub, but this is just a coincidence, we could have used any other API, like Twitter's or Facebook's).\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "Submit your assignment by following the steps above.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
